{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "02CbYlrmrReO"
      },
      "outputs": [],
      "source": [
        "!pip install portalocker>=2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torchdata\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpE3HiB5rU2q",
        "outputId": "124688bd-c394-4dd1-cb98-96624b8cefb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
            "Installing collected packages: torchdata\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.7.0\n",
            "    Uninstalling torchdata-0.7.0:\n",
            "      Successfully uninstalled torchdata-0.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.16.0 requires torchdata==0.7.0, but you have torchdata 0.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torchdata-0.7.1\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Installing collected packages: cloudpathlib, weasel, spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.7.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloudpathlib-0.16.0 spacy-3.7.2 weasel-0.3.4\n",
            "2024-01-23 20:13:20.311488: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-23 20:13:20.311558: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-23 20:13:20.313760: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-23 20:13:20.325407: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-23 20:13:22.040353: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.6.0\n",
            "    Uninstalling en-core-web-sm-3.6.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.6.0\n",
            "Successfully installed en-core-web-sm-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2024-01-23 20:13:38.235103: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-23 20:13:38.235175: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-23 20:13:38.236608: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-23 20:13:38.244723: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-23 20:13:39.526311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.3)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNgrTs3GrXTU",
        "outputId": "c6bd4d9a-0793-4eca-c7c3-28696aec2a7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.16.2-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Collecting torch==2.1.2 (from torchtext)\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.23.5)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchtext) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchtext) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchtext) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchtext) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchtext) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.2->torchtext)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.2->torchtext)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.2->torchtext)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2->torchtext)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.2->torchtext)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.2->torchtext)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.2->torchtext)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.2->torchtext)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.2->torchtext)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2->torchtext)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.2->torchtext)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->torchtext) (2.1.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->torchtext)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2->torchtext) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2->torchtext) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.16.0\n",
            "    Uninstalling torchtext-0.16.0:\n",
            "      Successfully uninstalled torchtext-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.1.2 torchtext-0.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkJHIcw6rcwf",
        "outputId": "fef38902-5609-4fcb-d53e-49102047545f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch\n",
            "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "\u001b[31mERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "yKpfllHGWIyG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "\n",
        "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "multi30k.URL[\"test\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt_task1_test2016.tar.gz\"\n",
        "multi30k.MD5[\"test\"] = \"876a95a689a2a20b243666951149fd42d9bfd57cbbf8cd2c79d3465451564dd2\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ],
      "metadata": {
        "id": "MmSnzTkUrghd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "def yield_tokens(data_iterator: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iterator:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "# UNK = unknown key\n",
        "# PAD = index used for padding in order to have flexible words length\n",
        "# SOS = start of sequence\n",
        "# EOS = end of sequence\n",
        "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    vocab_transform[language] = build_vocab_from_iterator(yield_tokens(train_iter, language),min_freq=1,specials=special_symbols, special_first=True)\n",
        "    vocab_transform[language].set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "id": "oNUZ_JVDrlTo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def add_end_and_start_tokens(token_ids):\n",
        "    return torch.cat((torch.tensor([SOS_IDX]),torch.tensor(token_ids),torch.tensor([EOS_IDX])))\n",
        "\n",
        "def transform_text_to_numbers(language, text):\n",
        "\n",
        "    #for token in  token_transform[language](text.rstrip(\"\\n\")):\n",
        "    tokens = token_transform[language](text.rstrip(\"\\n\"))\n",
        "    vocab_ids = vocab_transform[language](tokens);\n",
        "    text_transformed= add_end_and_start_tokens(vocab_ids)\n",
        "    return text_transformed\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def batch_transform(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(transform_text_to_numbers(SRC_LANGUAGE, src_sample))\n",
        "        tgt_batch.append(transform_text_to_numbers(TGT_LANGUAGE, tgt_sample))\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ],
      "metadata": {
        "id": "K-jJtlvvsN0D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masks(src, tgt):\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "    src_seq_len = src.shape[0]\n",
        "\n",
        "    tgt_mask = torch.triu(torch.ones((tgt_seq_len,tgt_seq_len),device = DEVICE)*float('-inf'),diagonal = 1)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).bool()\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "SpueYe26rp30"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,num_encoder_layers,num_decoder_layers,emb_size,nhead,src_vocab_size, tgt_vocab_size,dim_feedforward = 512,dropout = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.transformer = Transformer(d_model=emb_size,nhead=nhead,num_encoder_layers=num_encoder_layers,num_decoder_layers=num_decoder_layers,dim_feedforward=dim_feedforward,dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = nn.Embedding(src_vocab_size, self.emb_size)\n",
        "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, self.emb_size)\n",
        "        self.dropout = nn.Dropout(dropout);\n",
        "\n",
        "    def positional_embedding(self,emb_size,maxlen = 5000):\n",
        "          den = torch.exp(- torch.arange(0, self.emb_size, 2)* math.log(10000) / self.emb_size)\n",
        "          pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "          pos_embedding = torch.zeros((maxlen, self.emb_size))\n",
        "          pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "          pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "          pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "          return pos_embedding;\n",
        "\n",
        "    def forward(self,src,trg,src_mask,tgt_mask, src_padding_mask,tgt_padding_mask,memory_key_padding_mask):\n",
        "\n",
        "        src_token_emb = self.src_tok_emb(src)*math.sqrt(self.emb_size)\n",
        "        tgt_token_emb = self.tgt_tok_emb(trg)*math.sqrt(self.emb_size)\n",
        "        pos_emb = self.positional_embedding(self.emb_size);\n",
        "        src_emb = self.dropout(src_token_emb + pos_emb[:src_token_emb.size(0), :])\n",
        "        tgt_emb = self.dropout(tgt_token_emb + pos_emb[:tgt_token_emb.size(0), :])\n",
        "        out = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(out)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        src_token_emb = self.src_tok_emb(src)*math.sqrt(self.emb_size)\n",
        "        pos_emb = self.positional_embedding(self.emb_size);\n",
        "        out = self.transformer.encoder(src_token_emb + pos_emb[:src_token_emb.size(0), :], src_mask)\n",
        "        return out\n",
        "\n",
        "    def decode(self, tgt, memory, tgt_mask):\n",
        "        tgt_token_emb = self.tgt_tok_emb(tgt)*math.sqrt(self.emb_size)\n",
        "        pos_emb= self.positional_embedding(self.emb_size);\n",
        "        out= self.transformer.decoder(tgt_token_emb + pos_emb[:tgt_token_emb.size(0), :], memory,tgt_mask)\n",
        "\n",
        "        return self.generator(out)"
      ],
      "metadata": {
        "id": "0j_U49mzrusC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol, plot = True):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "\n",
        "    ys = (torch.ones(1, 1)*start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "\n",
        "        tgt_seq_len = ys.size(0);\n",
        "        tgt_mask = torch.triu(torch.ones((tgt_seq_len,tgt_seq_len),device = DEVICE)*float('-inf'),diagonal = 1).type(torch.bool).to(DEVICE)\n",
        "\n",
        "        all_probs = model.decode(ys, memory, tgt_mask)\n",
        "\n",
        "        prob = all_probs[-1,:,:]\n",
        "\n",
        "\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,(torch.ones(1, 1)*next_word).type_as(src.data)], dim=0)\n",
        "\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    return ys,all_probs"
      ],
      "metadata": {
        "id": "e2pvZsVEsk0n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = transform_text_to_numbers(SRC_LANGUAGE, src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens,_ = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=SOS_IDX)\n",
        "    tgt_tokens = tgt_tokens.flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "metadata": {
        "id": "OOdnUz4Sst4y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model,src_sentence):\n",
        "    model.eval()\n",
        "    src = transform_text_to_numbers(SRC_LANGUAGE, src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    ys,prob = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=SOS_IDX)\n",
        "    ys = ys.flatten()\n",
        "    words = torch.max(prob, dim = 2).indices\n",
        "    print(words)\n",
        "    print(ys)\n",
        "    return( \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(words.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))"
      ],
      "metadata": {
        "id": "nybWdpEE5YoJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_evaluation(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    val_iterator = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iterator, batch_size=BATCH_SIZE, collate_fn=batch_transform)\n",
        "    i = 0\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "        correct_predictions += torch.sum(predictions == tgt_out).item()\n",
        "        total_predictions += tgt_out.numel()\n",
        "\n",
        "    return losses / len(list(val_dataloader)),correct_predictions / total_predictions"
      ],
      "metadata": {
        "id": "G0nJuFSJ-Qrx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_evaluation(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    test_src = []\n",
        "    with open(drive_path_1+'test2016_de.txt') as mytxt:\n",
        "        for src_sentence in mytxt:\n",
        "            test_src+= [transform_text_to_numbers(SRC_LANGUAGE, src_sentence).view(-1, 1)]\n",
        "\n",
        "    test_tgt = []\n",
        "    with open(drive_path_1+'test2016_en.txt') as mytxt:\n",
        "        for tgt_sentence in mytxt:\n",
        "            test_tgt+= [transform_text_to_numbers(TGT_LANGUAGE, tgt_sentence).view(-1, 1)]\n",
        "\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "    #for src, tgt in test_dataloader:\n",
        "    for i in range(len(test_src)):\n",
        "        src = test_src[i]\n",
        "        tgt = test_tgt[i]\n",
        "\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "        correct_predictions += torch.sum(predictions == tgt_out).item()\n",
        "        total_predictions += tgt_out.numel()\n",
        "\n",
        "\n",
        "    return losses / i,correct_predictions / total_predictions"
      ],
      "metadata": {
        "id": "hBa5D0h9uhpn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluation(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    train_iterator = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iterator, batch_size=BATCH_SIZE, collate_fn=batch_transform)\n",
        "    i = 0\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "        correct_predictions += torch.sum(predictions == tgt_out).item()\n",
        "        total_predictions += tgt_out.numel()\n",
        "\n",
        "    return losses / len(list(train_dataloader)),correct_predictions / total_predictions"
      ],
      "metadata": {
        "id": "ujJZ7nNf_xa0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random\n",
        "def valid_bleau_score(model):\n",
        "    valid_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    indices = np.random.randint(0, len(srcs)-1, size=5)\n",
        "    bleu_scores = []\n",
        "    for i,sample in enumerate(valid_iter):\n",
        "\n",
        "        src = sample[0]\n",
        "        if(len(src) == 0):\n",
        "          continue;\n",
        "        tgt = sample[1]\n",
        "        hypothesis = token_transform[TGT_LANGUAGE](greedy_translate(model, src))[1:]\n",
        "        reference = token_transform[TGT_LANGUAGE](tgt)\n",
        "        if( i in indices):\n",
        "           print(i)\n",
        "           print(src)\n",
        "           print(hypothesis)\n",
        "           print(reference)\n",
        "           print('##########################')\n",
        "        bleu_score = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
        "        bleu_scores.append(bleu_score)\n",
        "    print(np.mean(np.array(bleu_scores)))"
      ],
      "metadata": {
        "id": "tFvaPO9_9Wm1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_bleu_score(model, srcs, tgts):\n",
        "    bleu_scores = []\n",
        "    for i in range(len(srcs)):\n",
        "\n",
        "        hypothesis = token_transform[TGT_LANGUAGE](greedy_translate(model, srcs[i]))[1:]\n",
        "        reference = token_transform[TGT_LANGUAGE](tgts[i])\n",
        "        bleu_score = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
        "        bleu_scores.append(bleu_score)\n",
        "    print(np.mean(np.array(bleu_scores)))"
      ],
      "metadata": {
        "id": "igYPvaM_9-xu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bleu_score(model):\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    bleu_scores = []\n",
        "    for i,sample in enumerate(train_iter):\n",
        "        src = sample[0]\n",
        "        if(len(src) == 0):\n",
        "          continue;\n",
        "        tgt = sample[1]\n",
        "        hypothesis = token_transform[TGT_LANGUAGE](greedy_translate(model, src))[1:]\n",
        "        reference = token_transform[TGT_LANGUAGE](tgt)\n",
        "        bleu_score = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
        "        bleu_scores.append(bleu_score)\n",
        "    print(np.mean(np.array(bleu_scores)))"
      ],
      "metadata": {
        "id": "osaASNz-_B1W"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_k_test_sentences(srcs,tgts,k = 5):\n",
        "    i = 0\n",
        "    indices =  np.random.randint(0, len(srcs)-1, size=5)\n",
        "    for i in indices:\n",
        "        print(i)\n",
        "        print(srcs[i])\n",
        "        print((greedy_translate(model, srcs[i])))\n",
        "        print(tgts[i])\n",
        "        print('##########################')\n"
      ],
      "metadata": {
        "id": "fEPVEVbE8Zr-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCy0pbQQsB96",
        "outputId": "ee6c0875-dd77-4527-82c0-d90ade797cd1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path_1 = '/content/drive/MyDrive/MyModels/Datasets/'\n",
        "srcs = []\n",
        "with open(drive_path_1+'test2016_de.txt') as mytxt:\n",
        "        for src_sentence in mytxt:\n",
        "             srcs.append(src_sentence)\n",
        "tgts = []\n",
        "with open(drive_path_1+'test2016_en.txt') as mytxt:\n",
        "        for tgt_sentence in mytxt:\n",
        "             tgts.append(tgt_sentence)"
      ],
      "metadata": {
        "id": "zHe_vDBgRnIs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path = '/content/drive/MyDrive/MyModels/Important/'"
      ],
      "metadata": {
        "id": "YoXwnyV87Xqu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ORIGINALAN MODEL**"
      ],
      "metadata": {
        "id": "Jnv5US-B64bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(drive_path+'transformer_model_0_16.pth')"
      ],
      "metadata": {
        "id": "Alui79zqsZZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "DIM_FEEDFORWARD = 512\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "UbJMd4zKsXCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(greedy_translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21yPRYExswxI",
        "outputId": "9a69c39b-b2fd-460c-ee9e-6c26b322e544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A group of people standing in front of an arena . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWpnEJCNAeLA",
        "outputId": "9ac30d5e-9dfd-4edc-d7e5-ad98f070f98d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8280804616764254, 0.3964013488397551)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANVKwQwctB1R",
        "outputId": "4e24386a-4035-4643-b287-93d94394261c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.9068016111850739, 0.32689983888292157)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sHqAIgybM2J",
        "outputId": "9c1cafd8-3415-491e-ea83-da596397dd30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.8561324387326374, 0.6762697396500214)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#traje previse dugo ( za 2.5h se nije izvrsilo idalje )\n",
        "#train_bleu_score(model)"
      ],
      "metadata": {
        "id": "H01qcNvmBGRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_bleau_score(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZURIAHwr-JQ-",
        "outputId": "947f27a3-1034-4f32-feca-6eba678f6657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66\n",
            "Ein Mann mit goldfarbener Kleidung steht neben seinem goldfarbenen Fahrrad.\n",
            "['A', 'man', 'in', 'golden', 'clothing', 'stands', 'next', 'to', 'his', 'gold', 'bike', '.']\n",
            "['A', 'man', 'is', 'wearing', 'a', 'gold', 'outfit', 'while', 'standing', 'with', 'his', 'gold', 'bike', '.']\n",
            "##########################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "258\n",
            "Eine Gruppe von Radfahrern legt sich bei einem Straßenrennen in die Kurve.\n",
            "['A', 'group', 'of', 'bicyclists', 'lay', 'down', 'a', 'road', 'in', 'a', 'road', 'race', '.']\n",
            "['A', 'pack', 'of', 'bicycle', 'road', 'racers', 'lean', 'through', 'a', 'curve', '.']\n",
            "##########################\n",
            "530\n",
            "Menschen in einer Schlange machen sich bereit, um in einen Bus zu steigen.\n",
            "['People', 'in', 'line', 'getting', 'ready', 'to', 'go', 'to', 'go', 'on', 'a', 'bus', '.']\n",
            "['People', 'in', 'line', 'getting', 'ready', 'to', 'board', 'a', 'bus', '.']\n",
            "##########################\n",
            "664\n",
            "Ein Mann schiebt Sessel auf einer belebten Straße.\n",
            "['A', 'man', 'pushing', 'a', 'chair', 'in', 'a', 'crowded', 'street', '.']\n",
            "['A', 'man', 'is', 'pushing', 'chairs', 'on', 'a', 'busy', 'street', '.']\n",
            "##########################\n",
            "969\n",
            "Eine Gruppe von Marinesoldaten geht mit amerikanischen Flaggen und anderen Militärflaggen die Straße entlang.\n",
            "['A', 'group', 'of', 'African', 'American', 'woman', 'walks', 'down', 'the', 'street', 'with', 'American', 'flags', 'and', 'other', 'flags', '.']\n",
            "['A', 'group', 'of', 'marines', 'walking', 'down', 'the', 'road', 'with', 'american', 'flags', 'and', 'other', 'military', 'flags', '.']\n",
            "##########################\n",
            "0.28853814398783106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_bleu_score(model,srcs,tgts)"
      ],
      "metadata": {
        "id": "oNpFyXW41YWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5652ef11-3723-4437-cf9c-9bd57c6f6e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.284865792038102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate_k_test_sentences(srcs,tgts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXVw-VeCBKSV",
        "outputId": "4b993b77-5fa2-4e51-bb82-988c2294c560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "674\n",
            "Ein Mann bückt sich und macht Hausarbeiten während im Hintergrund Tauben vorbeilaufen.\n",
            "\n",
            " A man is bending over and making a bowl with pigeons nearby . \n",
            "A man crouches while doing chores, as pigeons wander in the background.\n",
            "\n",
            "##########################\n",
            "416\n",
            "Der Junge ist im Freien und genießt einen Sommertag.\n",
            "\n",
            " The boy is outside and enjoying a day . \n",
            "The boy is outside enjoying a summer day.\n",
            "\n",
            "##########################\n",
            "225\n",
            "Eine Frau auf einem Boot namens \"El Corazon\" lässt schwarze Gewichte ins Wasser fallen.\n",
            "\n",
            " A woman in a boat named ELOIN the crest of the black dish is caught in the water . \n",
            "A woman on a boat named \"El Corazon\" drops black weights into the water.\n",
            "\n",
            "##########################\n",
            "334\n",
            "Eine Frau gibt einer Gruppe Anweisungen über einen Lautsprecher.\n",
            "\n",
            " A woman is giving a bunch of sheep over a German Shepherd . \n",
            "A woman directs a crowd of people with a loudspeaker.\n",
            "\n",
            "##########################\n",
            "616\n",
            "Ein Kind sitzt auf den Schultern einer Frau und klatscht.\n",
            "\n",
            " A child sits on a woman 's shoulders and clapping . \n",
            "A child claps while riding on a woman's shoulders.\n",
            "\n",
            "##########################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL SA 4 GLAVE**"
      ],
      "metadata": {
        "id": "3G-swuumylQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(drive_path+'transformer_model_head4_0_14.pth')"
      ],
      "metadata": {
        "id": "XJo9jyXLylQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 4\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "DIM_FEEDFORWARD = 512\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "sAAsNSzRylQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(greedy_translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5488acc5-cd31-4c8e-b693-54d17520f187",
        "id": "7NFanzcZzbVp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A group of people stand in front of an ATM machine . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dfbe8d4-7ada-42fd-f9ce-6e32abfbf40d",
        "id": "X7yvTWYIzbVp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9358285534224321, 0.3890526947048283)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c79eb478-dbcd-4eb7-c1b5-5e4f56e293f2",
        "id": "4u-ubEFXzbVq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.8930485993623734, 0.3261949516648765)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd670af-545e-40cb-dbda-63477cfb2225",
        "id": "oehAygYgzbVq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.8415694112355288, 0.6769810783895291)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#traje previse dugo ( za 2.5h se nije izvrsilo idalje )\n",
        "#train_bleu_score(model)"
      ],
      "metadata": {
        "id": "4pzAKwbAzbVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_bleau_score(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028a8f78-b8a4-4d1f-fe85-a6681b851727",
        "id": "n4O6wsrqzbVq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n",
            "Ein Hund geht mit einem Tuch im Maul durch einen kleinen Bach.\n",
            "['A', 'dog', 'walks', 'through', 'a', 'small', 'stream', 'with', 'a', 'small', 'stream', 'in', 'its', 'mouth', '.']\n",
            "['A', 'dog', 'walking', 'through', 'a', 'small', 'stream', 'with', 'a', 'rag', 'in', 'his', 'mouth', '.']\n",
            "##########################\n",
            "246\n",
            "Das ist eine belebte Kreuzung in einer Großstadt bei Nacht.\n",
            "['The', 'city', 'at', 'a', 'crowded', 'intersection', 'at', 'night', '.']\n",
            "['This', 'is', 'a', 'crowded', 'intersection', 'in', 'a', 'big', 'city', 'at', 'night', '.']\n",
            "##########################\n",
            "447\n",
            "Die Mitglieder einer Radsport-Mannschaft machen sich für ein Rennen bereit.\n",
            "['The', 'hockey', 'team', 'is', 'competing', 'for', 'a', 'race', '.']\n",
            "['A', 'crew', 'of', 'cycling', 'member', 'getting', 'ready', 'for', 'a', 'race', '.']\n",
            "##########################\n",
            "654\n",
            "Zwei Männer rauchen außerhalb eines Parks.\n",
            "['Two', 'men', 'smoking', 'out', 'in', 'a', 'park', '.']\n",
            "['Two', 'men', 'smoking', 'outside', 'of', 'a', 'park', '.']\n",
            "##########################\n",
            "772\n",
            "Ein Junge in einem schwarzen T-Shirt trägt einen blauen Eimer und geht neben weiß gekleideten Männern.\n",
            "['A', 'boy', 'wearing', 'a', 'black', 't', '-', 'shirt', 'and', 'blue', 'bucket', 'walks', 'next', 'to', 'men', 'wearing', 'white', '.']\n",
            "['A', 'boy', 'in', 'a', 'black', 'shirt', 'carries', 'a', 'blue', 'bucket', 'while', 'walking', 'with', 'men', 'dressed', 'in', 'white', '.']\n",
            "##########################\n",
            "0.29423266439702067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_bleu_score(model,srcs,tgts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb27dda-2ad9-44d4-dd05-09f4872f79ac",
        "id": "RErG13jZzbVq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.284508321628968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate_k_test_sentences(srcs,tgts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9ioSXdOzbVq",
        "outputId": "f7ded4f3-5bd3-4dc6-daed-7b1a813dfacb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "Eine Frau verwendet eine Bohrmaschine während ein Mann sie fotografiert.\n",
            "\n",
            " A woman uses an electric drill as a man takes a picture of them . \n",
            "A woman uses a drill while another man takes her picture.\n",
            "\n",
            "##########################\n",
            "704\n",
            "Ein Hund bettelt bei einem Mann und einer Frau.\n",
            "\n",
            " A dog shakes dog during a man and woman . \n",
            "A dog begging to a man and a woman.\n",
            "\n",
            "##########################\n",
            "487\n",
            "Weißer Hund auf einem Berg dreht sich zu etwas außerhalb des Bildes um, Himmel im Hintergrund.\n",
            "\n",
            " White dog on a mountain turning to get something in the background . \n",
            "White dog on mountainside turns to face something offstage, sky in background.\n",
            "\n",
            "##########################\n",
            "91\n",
            "Der Mann im japanischen Kochgewand bereitet ein Essen für zwei Personen zu.\n",
            "\n",
            " The man in a Japanese Japanese chef 's attire prepares to two people . \n",
            "The man in a Japanese cooking suit is preparing a meal for two people.\n",
            "\n",
            "##########################\n",
            "52\n",
            "Eine große Menschenmenge füllt eine Straße.\n",
            "\n",
            " A large crowd of people filling a street . \n",
            "A large group of people fill a street.\n",
            "\n",
            "##########################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aJS8vbeN3onK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL SA BROJEM GLAVA 16**"
      ],
      "metadata": {
        "id": "j72NDX-g3pEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(drive_path+'transformer_head16_model_0_17.pth')"
      ],
      "metadata": {
        "id": "uP0pwcE4zbVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 16\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "DIM_FEEDFORWARD = 512\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "he3tnmqyzbVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WYnUvTtzzbVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(greedy_translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d96d55e2-5ea4-4cc9-d25c-50f68c662453",
        "id": "zLH5fDUoylQW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A group of people stand in front of an igloo . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24bf54d-bed6-4927-dd05-13100b1628a0",
        "id": "0IoVBtlTylQX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7470813186158167, 0.4029932439024971)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8c969b-a63c-4ef1-d425-6496393082e0",
        "id": "MDH64rdkylQX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.9432875365018845, 0.32582572502685286)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "581f4f74-fb5b-4c6f-d44e-9bd5537d8705",
        "id": "PiN4W4nnylQX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.9046275918824118, 0.6739223218096457)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#traje previse dugo ( za 2.5h se nije izvrsilo idalje )\n",
        "#train_bleu_score(model)"
      ],
      "metadata": {
        "id": "N10sRRV2ylQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_bleau_score(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28454a8d-7cfd-4d41-dcda-ea30e4552cb4",
        "id": "LZ9-K6DnylQX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "305\n",
            "In einer Industrieumgebung werden große, weiche Scheiben von einem Mann auf einer Spindel gestapelt, während ein anderer Mann zusieht.\n",
            "['In', 'a', 'heavily', 'modern', 'class', ',', 'including', 'a', 'man', 'in', 'an', 'olive', '-', 'like', 'device', ',', 'while', 'another', 'man', 'watches', '.']\n",
            "['Soft', 'large', 'discs', 'are', 'being', 'stacked', 'on', 'a', 'spindle', 'in', 'an', 'industrial', 'environment', 'by', 'one', 'man', 'as', 'another', 'looks', 'on', '.']\n",
            "##########################\n",
            "348\n",
            "Ein Junge und ein alter Mann mit einem Stock unterhalten sich.\n",
            "['A', 'boy', 'and', 'an', 'old', 'man', 'talking', 'to', 'a', 'cane', '.']\n",
            "['A', 'boy', 'and', 'an', 'old', 'man', 'with', 'a', 'cane', 'are', 'talking', '.']\n",
            "##########################\n",
            "510\n",
            "Ein Motocrossfahrer wird bei einem Sprung auf einer Rennstrecke leicht durch die Luft getragen.\n",
            "['A', 'motocross', 'rider', 'is', 'being', 'lifted', 'through', 'the', 'air', 'on', 'a', 'track', 'in', 'a', 'race', '.']\n",
            "['A', 'motocross', 'rider', 'is', 'slightly', 'airborne', 'on', 'a', 'competition', 'circuit', 'jump', '.']\n",
            "##########################\n",
            "603\n",
            "Zwei einander zugewandte Menschen spielen ein Spiel.\n",
            "['Two', 'female', 'beach', 'volleyball', 'players', 'are', 'playing', 'a', 'game', '.']\n",
            "['Two', 'people', 'playing', 'a', 'game', 'facing', 'each', 'other', '.']\n",
            "##########################\n",
            "750\n",
            "Ein Mann überlegt in seinem Wohnzimmer, was er für eine Reise einpacken soll.\n",
            "['A', 'man', 'catches', 'what', 'to', 'be', 'a', 'meal', 'in', 'his', 'living', 'room', '.']\n",
            "['A', 'man', 'in', 'his', 'living', 'room', 'ponders', 'packing', 'for', 'a', 'trip', '.']\n",
            "##########################\n",
            "0.2846093760373506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_bleu_score(model,srcs,tgts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a46a69-151a-45a9-c7b3-5d212c1574a8",
        "id": "MwTMvc_FylQX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.276631396318938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate_k_test_sentences(srcs,tgts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nVaU-QHylQX",
        "outputId": "2be46d93-8a13-475d-dfd5-54d773c31dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "697\n",
            "Ein Mann in einem roten Anzug tanzt mit einer Dame.\n",
            "\n",
            " A man in a red suit dances with a lady . \n",
            "A man with a red suit is dancing with a lady.\n",
            "\n",
            "##########################\n",
            "627\n",
            "Ein Mann in einer weißen Schürze bereitet für eine Frau in einer weißen Jacke im Freien in einer Pfanne etwas mit Eiern zu.\n",
            "\n",
            " A man in a white apron is preparing for a woman in a white jacket outside in a pan . \n",
            "A man in a white apron is cooking something with eggs on a pan outside for a woman in a tan jacket.\n",
            "\n",
            "##########################\n",
            "6\n",
            "Eine Gruppe von Menschen steht vor einem Iglu.\n",
            "\n",
            " A group of people stand in front of an igloo . \n",
            "A group of people standing in front of an igloo.\n",
            "\n",
            "##########################\n",
            "105\n",
            "Ein Bergsteiger mit einem blauen Helm beginnt mit dem Abstieg.\n",
            "\n",
            " A rock climber with a blue helmet begins to pass the batter . \n",
            "A mountaineer about to descend down a mountain with a blue helmet on.\n",
            "\n",
            "##########################\n",
            "772\n",
            "Drei Mädchen lächeln für ein Foto.\n",
            "\n",
            " Three girls smile for a picture . \n",
            "Three girls are smiling for a picture.\n",
            "\n",
            "##########################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xgqp318TylQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL SA BROJEM dimenzija 256**"
      ],
      "metadata": {
        "id": "i8Wjr1P28RR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(drive_path+'transformer_dim256_model_0_19.pth')"
      ],
      "metadata": {
        "id": "iJ0Eunx98RR6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "EMB_SIZE = 256\n",
        "NHEAD = 4\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "DIM_FEEDFORWARD = 256\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "GJ17xfuv8RR7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lmMujqmg8RR7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(greedy_translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccf1977c-1365-434a-ba6c-e7f1b6632f08",
        "id": "3wkFeauB8RR7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A group of people are standing in front of an urban area . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d95ea9-e56a-4593-f3d9-3714bf5a8261",
        "id": "nnNwN41a8RR7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.2754820845200627, 0.3604088878735308)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf800f0-1c86-4dd6-c438-e63a7ad922ff",
        "id": "1syTAJ2K8RR7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.0256721526384354, 0.3194146079484425)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94bba4d-8d3a-407d-8eaa-7c7145842819",
        "id": "VnL86Hmc8RR7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.9782022604787792, 0.6584862711623275)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#traje previse dugo ( za 2.5h se nije izvrsilo idalje )\n",
        "#train_bleu_score(model)"
      ],
      "metadata": {
        "id": "drUEEpzY8RR8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_bleau_score(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba5638f5-26f1-41cf-be1a-b0abf6242a4c",
        "id": "BsbuOgIM8RR8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44\n",
            "Ein Mann fährt ein altmodisches rotes Rennauto.\n",
            "['A', 'man', 'is', 'riding', 'a', 'red', 'race', 'race', '.']\n",
            "['A', 'man', 'drives', 'an', 'old', '-', 'fashioned', 'red', 'race', 'car', '.']\n",
            "##########################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "216\n",
            "Eine Band spielt auf dem Gehweg.\n",
            "['A', 'band', 'is', 'playing', 'on', 'the', 'sidewalk', '.']\n",
            "['A', 'band', 'playing', 'on', 'a', 'sidewalk', '.']\n",
            "##########################\n",
            "330\n",
            "Ein Mann beim Wakeboarden im Wasser.\n",
            "['A', 'man', 'is', 'flying', 'in', 'the', 'water', '.']\n",
            "['A', 'man', 'wakeboards', 'in', 'the', 'water', '.']\n",
            "##########################\n",
            "812\n",
            "Ein Mann in einer grauen Jacke transportiert Laub auf einem Fahrrad.\n",
            "['A', 'man', 'in', 'a', 'gray', 'jacket', 'with', 'leaves', 'on', 'a', 'bicycle', '.']\n",
            "['A', 'man', 'on', 'a', 'bike', 'in', 'a', 'gray', 'jacket', 'carries', 'foliage', '.']\n",
            "##########################\n",
            "994\n",
            "Ein Kickboxer landet einen Knietreffer im Gesicht seines Gegners.\n",
            "['A', 'biker', 'is', 'flying', 'through', 'the', 'back', 'of', 'a', 'racetrack', 'in', 'his', 'mouth', '.']\n",
            "['A', 'kickboxer', 'lands', 'a', 'flying', 'knee', 'into', 'the', 'face', 'of', 'his', 'opponent', '.']\n",
            "##########################\n",
            "0.2661367494441856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_bleu_score(model,srcs,tgts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "863f2115-cb4d-456c-b985-c4f99d0d21c7",
        "id": "m85fePDs8RR8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2544239355659486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate_k_test_sentences(srcs,tgts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceeb3c40-3236-42fe-eb50-51ca9144d180",
        "id": "BcPiorZQ8RR8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "648\n",
            "Eine junge brünette Frau isst und trinkt etwas.\n",
            "\n",
            " A young brunette woman is eating something and drinking . \n",
            "A young brunette woman eating and drinking something.\n",
            "\n",
            "##########################\n",
            "100\n",
            "Eine glückliche Frau bereitet in einem Coffee-Shop eine Erfrischung zu.\n",
            "\n",
            " A happy woman prepares a telescope in a hospital . \n",
            "A happy woman is preparing a refreshment at a coffee shop.\n",
            "\n",
            "##########################\n",
            "793\n",
            "Ein Mann fährt ein Allradfahrzeug mit vier Passagieren vorne und einem seitlich sitzenden Mann hinten.\n",
            "\n",
            " A man is riding a four - haired , four sitting on the beach with four seated man . \n",
            "A man driving a four wheeled vehicle with four passengers riding on the front and a man sitting sideways on the back.\n",
            "\n",
            "##########################\n",
            "717\n",
            "Ein Mann auf einem Rennpferd mit anderen Männern auf Pferden hinter ihm.\n",
            "\n",
            " A man on a track with another men on horses behind him . \n",
            "A man riding a running horse with some other men doing the same behind him.\n",
            "\n",
            "##########################\n",
            "282\n",
            "Ein blondes Kind schaukelt auf einer Schaukel.\n",
            "\n",
            " A blond child is swinging on a swing . \n",
            "A blond child swinging on a swing.\n",
            "\n",
            "##########################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL SA slojem dimenzija 1024**"
      ],
      "metadata": {
        "id": "r5nIq_KO-J-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(drive_path+'transformer_model_dim1024_0_11.pth')"
      ],
      "metadata": {
        "id": "-hVgIEgG-J-F"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "EMB_SIZE = 1024\n",
        "NHEAD = 4\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "DIM_FEEDFORWARD = 1024\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "Pz77c7Nt-J-F"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w84kNdCG-J-F"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(greedy_translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nKICO4P-J-F",
        "outputId": "b1c1fba2-34f7-48a8-d4c4-3d0c3795e509"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A group of people stand in front of an open area . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYirQXqk-J-F",
        "outputId": "b61d678e-56c9-44de-b557-8b0dd2c94690"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6673381894433026, 0.4113639987339754)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxJPYq5g-J-F",
        "outputId": "d50fa694-4eee-4490-e80a-171597c0c5b9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.8069382458925247, 0.32945085929108486)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_evaluation(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjoRpFaG-J-G",
        "outputId": "92862039-8355-479c-f077-c461a6ecebe9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.7960672918129135, 0.6825295205576896)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#traje previse dugo ( za 2.5h se nije izvrsilo idalje )\n",
        "#train_bleu_score(model)"
      ],
      "metadata": {
        "id": "Q-FvFFcQ-J-G"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_bleau_score(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCTLHxAs-J-G",
        "outputId": "1facd5df-97fe-45c5-b720-a22204320fab"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "Ein Mann mit beginnender Glatze, der eine rote Rettungsweste trägt, sitzt in einem kleinen Boot.\n",
            "['A', 'balding', 'man', 'wearing', 'a', 'red', 'life', 'jacket', 'sits', 'in', 'a', 'small', 'boat', '.']\n",
            "['A', 'balding', 'man', 'wearing', 'a', 'red', 'life', 'jacket', 'is', 'sitting', 'in', 'a', 'small', 'boat', '.']\n",
            "##########################\n",
            "662\n",
            "Zwei Menschen sitzen auf einer Bank aus Beton und plaudern beim Mittagessen.\n",
            "['Two', 'people', 'sit', 'on', 'a', 'bench', 'and', 'chatting', '.']\n",
            "['Two', 'people', 'sitting', 'on', 'a', 'cement', 'bench', 'chatting', 'over', 'lunch', '.']\n",
            "##########################\n",
            "865\n",
            "Eine korpulente Frau trocknet ihr Haar mit einem Föhn und lächelt dabei glücklich\n",
            "['A', 'flower', 'woman', 'is', 'interacting', 'with', 'her', 'hair', 'and', 'smiles', 'as', 'she', 'smiles', 'happily', '.']\n",
            "['Heavyset', 'woman', 'blowing', 'her', 'hair', 'with', 'a', 'hair', 'dryer', 'smiling', 'all', 'happy']\n",
            "##########################\n",
            "962\n",
            "Zwei Kickboxerinnen, von denen eine einen lilafarbenen Sport-BH trägt, kämpfen in einer Arena.\n",
            "['Two', 'flower', 'girls', ',', 'one', 'of', 'them', 'is', 'wearing', 'a', 'purple', 'sports', 'sports', 'sports', 'sports', 'game', '.']\n",
            "['Two', 'female', 'kickboxers', ',', 'one', 'with', 'a', 'purple', 'sports', 'bra', ',', 'battle', 'it', 'out', 'in', 'an', 'arena', '.']\n",
            "##########################\n",
            "994\n",
            "Ein Kickboxer landet einen Knietreffer im Gesicht seines Gegners.\n",
            "['A', 'street', 'driver', 'is', 'landing', 'a', 'bicycle', 'race', 'with', 'his', 'opponent', 'in', 'the', 'back', '.']\n",
            "['A', 'kickboxer', 'lands', 'a', 'flying', 'knee', 'into', 'the', 'face', 'of', 'his', 'opponent', '.']\n",
            "##########################\n",
            "0.29816784778006794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_bleu_score(model,srcs,tgts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUdrc_i1-J-G",
        "outputId": "207b47c6-171e-4885-d82f-209118aff355"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2945922338698585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate_k_test_sentences(srcs,tgts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puSADwfW-J-G",
        "outputId": "52e0c009-b1c3-447d-c1bc-bd1591fb13c4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "727\n",
            "Ein Mann mit einer schwarzen Weste hält ein Modellflugzeug\n",
            "\n",
            " A man with a black vest is holding a model airplane . \n",
            "A man with a black vest holding a model airplane\n",
            "\n",
            "##########################\n",
            "154\n",
            "Zwei Personen fahren auf Fahrrädern durch eine Gebirgslandschaft.\n",
            "\n",
            " Two people are riding bicycles through a bright race . \n",
            "Two people riding bikes through a mountainous region.\n",
            "\n",
            "##########################\n",
            "318\n",
            "Kleine Kinder fahren in einem Miniaturzug.\n",
            "\n",
            " Young children riding on a train station . \n",
            "Young kids are on a small train ride.\n",
            "\n",
            "##########################\n",
            "582\n",
            "Santa Claus wird bei einem Medienevent zu Weihnachten fotografiert.\n",
            "\n",
            " <unk> band being photographed by a bright yellow car . \n",
            "Santa Claus being photographed at a holiday media event.\n",
            "\n",
            "##########################\n",
            "553\n",
            "Eine Gruppe von Männern in blauen Uniformen stehen zusammen.\n",
            "\n",
            " A group of men in blue uniforms stand together . \n",
            "A group of men in blue uniforms are standing together.\n",
            "\n",
            "##########################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kpVuBmqmD5HT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}